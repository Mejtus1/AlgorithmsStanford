Asymptotic Analysis 
- provides basic vocabulary for discussing the design and analysis of algorithms
- it is a sweet spot for high level reasoning about algorithms 
- coarsse enough to suppress architecture/language compiler dependent details 
- sharp enough to make algorithms useful comparisons between different algorithms, especially large inputs (sorting, integer multiplication)
- high level programmes will tell you, asymptotic analysis supresses constant factors and lower order terms
- lovel order terms become irrelevant for large inputs 
- constant factors are too dependent on the environment 

Big O notation examples 

Example 1 :
- equate 6n log2n + 6n with just n log n 
6n log2n +6n 
- we drop 6n with suppression and we are left with much simplier expression n klog(n)
where n = input size 

Example 2:
- does array A contain integer t ? Given A = array of length n, t = an integer 
for i = 1 to n 
    if A[i] == t + return TRUE 
return FALSE 
Question: What is the running time ? 
O(n) - running time of algorithm is linear with input length n 

Example 3: 
- given A,B arrays of length n and t = an integer 
- does A or B contain t ? 
for i = 1 to n 
    if A[i] == t + return TRUE 
for i = 1 to n 
    if B[i] == t + return TRUE 
return FALSE 
O(n) - linear algorithm

Example 4: 
- two nested loops 
- do arrays A,B have a number in common ? given arrays A,B length n 
for i = 1 to n 
  for j = 1 to n 
    if A[i] == B[j] return TRUE 
return FALSE 
running time ? = O(n2) squared = quadratic running time algorithm
- with each iteration of outer for loop, we do each iteration of inner for loop, this gives us running time of n2 suquared 

Example 5: 
- does array A have duplicate entries ? given array A of length n 
- althrough for loops look similar, this time we look for duplicate values 
- i + 1 = because otherwise we would compare each element inside arrays twice, we need to only compare them once 
for i = 1 to n 
  for j = i + 1 to n 
    if A[i] == A[j] return TRUE 
return FALSE 
running time ? 
running time ? = O(n2) squared = quadratic running time algorithm

// --------------------------------------------------------------------------------------
Bih-Oh Notation 
- let T(n) = function on n = 1,2,3,.... [usually worst case running time of an algorithm]

When is T(n) = O(f(n)) ? 
- for all suffiecently large values of n, T(n) is bounded above by constant multiple of f(n)

Example: 
(graphs)
T(n) on graph 
F(n) on graph, below T(n)
- but when we double 2f(n), it crosses T(n) and forever is larger than T(n)
- in this event constant multiple times of f(n), twice f(n), is upper bound of T(n)
Formal definition in mathematics: 
T(n) = O(f(n)) if and only if there exist constants c, no > O so that T(n) <= c f(n)
for all n >= no 
c = constant multiple of f(n)
no = quantyfing suffiecently large 

- in upper example: 
c = 2 
no (n o) = crossing point where 2F(n) and T(n) cross 

Warning: c, no cannot depend on n

