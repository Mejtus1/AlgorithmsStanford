// --------------------------------------------------------------------------------------------
Asymptotic Bounding 

- single for loop = linear time ? O(n) ? 
for(int i = 0; i < n; i++) {
}
- 2 for loops = quadratic time ? O(n^2) ? 
for(int i = 0; i < n; i++) {
    for(int j = 0; j < 10; j++) {
}
}
- not always 
- we can only see how well algorithm works, with very large inputs 

Insertion Sort: 2 n2 
Merge Sort: 50 n log(n)

Computer A: 10,000,000,000 instructions/second
Computer B: 10,000,000     instructions/second 

Computer A Time: 5.5 hours 
Computer B Time: 20 minutes 

- althrough computer B is 1000 times slower, it is able to perform much faster than computer 1

Linear Time O(n)
- we have a graph and inputs: y=1/2x, y=x, y= 2x, y=3x
- on graph we can see that all of the inputs have same behavior 

Asymptotic complexity 
- whole point of taking asymptotic measures is so that we know how a graph is behaving
when input is very large, that is when we see graphs true colors 
- this is also reason why we drop constants O(1/2*n), we drop off 1/2 
- that is because all of these inputs in graphs will perform in very same manner 
with very very large inputs 

log n time: O(log(n))
linear time: O(n)
quadratic time: O(n^2)
qubic time: O(n^3)
factorial time: O(n!)
exponential time: O(2^n)
- we are describing a class of functions, a class of behaviors 

// ----------------------------------------------------------------------------------------------
Asymptotic Bounds 
- basic way asymtotic bounds work, is: 

O(n)
- everytime we have piece of code or algorithm, they consume resources
- these resources are Time, or Space 
- our rob is to analyze how much job does this algorithm take 
- more we understand how our algorithm behaves, functions and is made of 
more we can improve it 

Asymptotic 
- nature of a graph or function as it reaches untouchable bound
- very large n value can show us how our algorithm behaves with unimaginable values 
T(n) = function, when we move n up or down, the output of that function changes 
T = Time 
n = space 

Big O 
- it is upperbound, upperbounding work 
- mathematical definition of big O 
T(n)            
is O(f(n)) iff - T of n is upperbounded if
T(n) <= c*f(n) - T of n is less than or equal to some constant c * f(n), the function we choose to bound with 
for all n >= n 
WHAT DOES THIS MEAN ? 

- we have class of functions of how we can bound 
log n time: O(log(n))
linear time: O(n)
quadratic time: O(n^2)
qubic time: O(n^3)
factorial time: O(n!)
exponential time: O(2^n)
- example: 
- defining a function that might bound T(n) lower on the graph than 
n2 vs T(n) 
f(n) = n2
- raw n2 function multiplied by c = 1 constant, it is lowerbound than T(n) on graph 
- now are we able to modulate 1*n2 to that T(n) is in constant state under it on graph ? 
c = 100
f(n) = n2 
- if we take all n values, will T(n) always stay below n2 bounding function ? 
- yes 
This function is upperbounded

Bounding and Tight
- when we perform bounding we want to be as tight and exact as possible to dont have all extra 
wasted space between T(n) and function 
- O(n) function 
f(n) = n 
c = 1 
- T(n) is still below T(n)
c = 1000
- T(n) is upperbound, but this is big value 
c = 10 
- is lower than 1000, but it is upperbound to T(n)
- the point is proven 
- this is a bound to the order of linear time 
- behavior of our function is set, we can change c 

1. we choose our base function from: 
log n time: O(log(n))
linear time: O(n)
quadratic time: O(n^2)
qubic time: O(n^3)
factorial time: O(n!)
exponential time: O(2^n)
2. find a good c value, constant and prove that T(n) stays under it 
3. and our bound works,
- that is valid upper bound, wheather it is loose or tight, althrough we prefer tight 

- another example: 
T(n) = log(n)
c = 1 
1 * log(n)
- lower bound 
c = 10 
10 * log(n)
- lower bound
c = 100
100*f(n)
- lower bound 
- because of logarithm, this cannot be bound as upperbound never
- because we cannot bound linear time with log(n) time, because at some point linear will fail 

Why do we drop constants ? 
- O(2n) doesnt make sense to have constants, because we can choose any c value 

Other bounds
Lower bound 
- we use lower bound OMEGA 
- we want to know what is the least amount of work we can do 
- we need to stay T(n) all the time 

Theta 
- tight bound or exact bound 
- we take upperbound and lowerbound and we have theta
- if we have T(n) we need to find a f(n) that bounds it from the top and bottom 
n2 
- is upperbound but not lowerbound 
f(n) = n 
c = 100     - upperbound 
c = 1/2*2   - lowerbound 
- this might not be accurate to scale, but what we see is 
- we satisfied O() constraints upper bound 
- we satisfied Omega constraints lower bound 
- we proved that f(n) is fit function, because of our ability to choose correct c values
- we can tightly bound to the linear order of time, where f(n) is upperbound and lowerbound 

