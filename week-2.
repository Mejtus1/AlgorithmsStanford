// --------------------------------------------------------------------------------------------
Asymptotic Bounding 

- single for loop = linear time ? O(n) ? 
for(int i = 0; i < n; i++) {
}
- 2 for loops = quadratic time ? O(n^2) ? 
for(int i = 0; i < n; i++) {
    for(int j = 0; j < 10; j++) {
}
}
- not always 
- we can only see how well algorithm works, with very large inputs 

Insertion Sort: 2 n2 
Merge Sort: 50 n log(n)

Computer A: 10,000,000,000 instructions/second
Computer B: 10,000,000     instructions/second 

Computer A Time: 5.5 hours 
Computer B Time: 20 minutes 

- althrough computer B is 1000 times slower, it is able to perform much faster than computer 1

Linear Time O(n)
- we have a graph and inputs: y=1/2x, y=x, y= 2x, y=3x
- on graph we can see that all of the inputs have same behavior 

Asymptotic complexity 
- whole point of taking asymptotic measures is so that we know how a graph is behaving
when input is very large, that is when we see graphs true colors 
- this is also reason why we drop constants O(1/2*n), we drop off 1/2 
- that is because all of these inputs in graphs will perform in very same manner 
with very very large inputs 

log n time: O(log(n))
linear time: O(n)
quadratic time: O(n^2)
qubic time: O(n^3)
factorial time: O(n!)
exponential time: O(2^n)
- we are describing a class of functions, a class of behaviors 

// ----------------------------------------------------------------------------------------------
Asymptotic Bounds 
- basic way asymtotic bounds work, is: 

O(n)
- everytime we have piece of code or algorithm, they consume resources
- these resources are Time, or Space 
- our rob is to analyze how much job does this algorithm take 
- more we understand how our algorithm behaves, functions and is made of 
more we can improve it 

Asymptotic 
- nature of a graph or function as it reaches untouchable bound
- very large n value can show us how our algorithm behaves with unimaginable values 
T(n) = function, when we move n up or down, the output of that function changes 
T = Time 
n = space 

Big O 
- it is upperbound, upperbounding work 
- mathematical definition of big O 
T(n)            
is O(f(n)) iff - T of n is upperbounded if
T(n) <= c*f(n) - T of n is less than or equal to some constant c * f(n), the function we choose to bound with 
for all n >= n 
WHAT DOES THIS MEAN ? 

- we have class of functions of how we can bound 
log n time: O(log(n))
linear time: O(n)
quadratic time: O(n^2)
qubic time: O(n^3)
factorial time: O(n!)
exponential time: O(2^n)
- example: 
- defining a function that might bound T(n) lower on the graph than 
n2 vs T(n) 
f(n) = n2
- raw n2 function multiplied by c = 1 constant, it is lowerbound than T(n) on graph 
- now are we able to modulate 1*n2 to that T(n) is in constant state under it on graph ? 
c = 100
f(n) = n2 
- if we take all n values, will T(n) always stay below n2 bounding function ? 
- yes 
This function is upperbounded

Bounding and Tight
- when we perform bounding we want to be as tight and exact as possible to dont have all extra 
wasted space between T(n) and function 
- O(n) function 
f(n) = n 
c = 1 
- T(n) is still below T(n)
c = 1000
- T(n) is upperbound, but this is big value 
c = 10 
- is lower than 1000, but it is upperbound to T(n)
- the point is proven 
- this is a bound to the order of linear time 
- behavior of our function is set, we can change c 

1. we choose our base function from: 
log n time: O(log(n))
linear time: O(n)
quadratic time: O(n^2)
qubic time: O(n^3)
factorial time: O(n!)
exponential time: O(2^n)
2. find a good c value, constant and prove that T(n) stays under it 
3. and our bound works,
- that is valid upper bound, wheather it is loose or tight, althrough we prefer tight 

- another example: 
T(n) = log(n)
c = 1 
1 * log(n)
- lower bound 
c = 10 
10 * log(n)
- lower bound
c = 100
100*f(n)
- lower bound 
- because of logarithm, this cannot be bound as upperbound never
- because we cannot bound linear time with log(n) time, because at some point linear will fail 

Why do we drop constants ? 
- O(2n) doesnt make sense to have constants, because we can choose any c value 

Other bounds
Lower bound 
- we use lower bound OMEGA 
- we want to know what is the least amount of work we can do 
- we need to stay T(n) all the time 

Theta 
- tight bound or exact bound 
- we take upperbound and lowerbound and we have theta
- if we have T(n) we need to find a f(n) that bounds it from the top and bottom 
n2 
- is upperbound but not lowerbound 
f(n) = n 
c = 100     - upperbound 
c = 1/2*2   - lowerbound 
- this might not be accurate to scale, but what we see is 
- we satisfied O() constraints upper bound 
- we satisfied Omega constraints lower bound 
- we proved that f(n) is fit function, because of our ability to choose correct c values
- we can tightly bound to the linear order of time, where f(n) is upperbound and lowerbound 

// ----------------------------------------------------------------------------------------------
// BIG O NOTATION and Time complexities 
- how does algorithm speed scale and input scale when becoming very large 
- example O(n2 squared) vs O(n): at the beginning there is not much difference,
but after input gets larger, the O(n) scales much better than O(n2) squared
- it is called asymtotic complexity, because we are looking at the behavior of graph as input 
scales very large
- we know that linear O(n) is never gonna pass O(n2) function 
- mistake is when we have 2 inputs, we need to factor them in (m, n)
- min, max notation when we want wo use larger, smaller string, input 
O(max(m,n)) - O of max m,n 
O(min(m,n)) - O of min m,n

Time Complexity 
O(1) "constant time" - as algorithm gets very large input, runtime is the same 
- in Big O we worry about tail behavior on graph 

1. O(log(n)) "Logarithmic Time"
- normally log by 2 
- looks like logarithm on a graph 
log(16) = 4, because 2 to the power of 4 is 16 == halving

Binary search 
- balanced binary search 
- search tree traversal

2. O(n) "Linear Time"
- common 
- n tree notes, n array elements

3. O(n*log(n))
What does this mean ? - 3 levels of work (log levels of work, for each level we have)
- fast sorting 
- merge sort, quick sort 

Merge sort 
log(8) = 3 
n = [1,3,2,5,7,8,2,9]
n = 8 
      [1,3,2,5] [7,8,2,9]         1. 
    [1,3] [2,5] [7,8] [2,9]       2. 
[1] [3] [2] [5] [7] [8] [2] [9]   3. 

4. O(n2) squared 
- naive solution 
- naive sorting algorithms: bubble sort, selection sort, insertion sort 

[1,2,3,4]
 3 2 1 0 - comparisons 
O(n2/2) = O 1/2 n2 = O(n squared) - because constant gets dropped 

5. O(2 power of n) "Exponential Time"
- backtracking problems, subsets 

6. O(n!) "n-fractional"
- permutations 

       " cat "
    |     |      | 
c "at"  a "ct"  t "ca"        3. forks 
 ---     ---     ---             .
|     |   ................       . 
ca"t" ct"a"                   2. forks 
                                 .
                              1. factorial
3 x 2 x 1 = 3! factorial = n! - we get n factorial time here for permutation problem 

Space Complexity and Time Complexity
- optimizing time or space 
- we can lower time complexity and up space complexity
- we can increace time complexity and lower space (do more work, keep less auxilary space)
- what makes sense ? we can buy memory, but we cant buy time = best trade of is increase SPACE, lower TIME 

Space Complexity 
- How does the space usage scale as our input gets very large ? 
- What auxilary space does your algorithm use ? 

example: 
sorting arrays = log(n) = binary search 

// ----------------------------------------------------------------------------------------------
Permutations 

A,B,C,D,E
- - - - - 
1 2 3 4 5 
- means how many combinations of putting these letters into brackets we have 
- - - - - 
5 posibilities from A,B,C,D,E for 1st brackets
5 - - - - 
4 posibilities from A,B,C,D,E for second bracket
5 4 - - - 
...
- so the number of permutation is 5! = 5 factorial = 5x4x3x2x1 = 120 

- now we have 5 letters 
A,B,C,D,E - and only 3 brackets 

- - - 
1 2 3 
- how many letters can we put in each bracket ? 
- - - 
5 4 3 = 5 x 4 x 3 = 60 permutations 
- but how do we define our factorial when we only use 3 brackets and have 5 letters
     5 x 4 x 3 x 2 x 1     5 x 4 x 3       5! 
60 = ----------------- = -------------- = ----
                              2 x 1        2!
- we take the rest of parmutation which is 2 x 1 and place it denominator 
               n! 
P(n,r), nPr = ------
              (n-r)! 
P = letters
n = brackets 
- most of the time people reason permutations through and dont use a formula 
- in permutations the order matters, 

A B C D 
- - 
- we are looking how many different ways can we arrange and combine 4 letters 

AB BA CA DA
AC BC CB DB
AD BD CD DC

- there are 12 different ways we can differentiate 2 out of 4 letters
permutations = 12 
P = 12 
        n!
nPr = --------
       (n-R)! 
         4!        4!
4P2 = --------- = -----
        (4-2)      2!
- in longer factorial terms: 
         4!        4x3x2x1         4x3x 
4P2 = --------- = ----------- = ------------ = 12 
        (4-2)         2x1 
